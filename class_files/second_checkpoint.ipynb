{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Supreme Court Oral Arguments Outcome Prediction Team</center></h2>\n",
    "<h2 style=\"margin-top: -15px; margi-bottom: 10px;\"><center>Second Checkpoint</center></h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project and Team Information\n",
    "**Team members:** Federico Dominguez Molina, Jessup Jong, Chanteria Milner, and Michael Plunkett\n",
    "\n",
    "**Project summary:** The project uses historic United States Supreme Court cases to train natural language processing models to predict case rulings.\n",
    "**Project repository:** [Link](https://github.com/michplunkett/supreme-court-ml-predictions)\n",
    "**Project assumptions and things to know:**\n",
    "1. The number of unique roles within the advocates' file is too numerous to be helpful, so we merged them into 5 categories. While this merger may remove some variability and nuance in the file, we believe it will make it easier to derive meaningful conclusions.\n",
    "    - The groupings for the roles are as follows: inferred, for respondent, for partitioner, and for amicus curiae\n",
    "2. The years included within this data set are 2014 to 2019.\n",
    "3. The datasets included within the previously mentioned year range are ones where the winnings side was either 0 or 1 (no missing, etc.).\n",
    "\n",
    "**Note:** If you would like any examples of code that we used, they are all on the bottom of the page the section titled `Code Examples`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We process the data through three steps:\n",
    "1. [Michael: Gonna fill this area out before 6 PM]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[Michael: I am going to add a line for each csv file so they can see what data is in it]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Models: Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**What is logistic regression?**\n",
    "\n",
    "\n",
    "Logistic regression is a supervised machine learning model which, based on the training data, predicts the probability of a binary outcome. This is a supervised learning approach because the model is trained on data where the outcome is known. \n",
    "\n",
    "Logistic regression is also a classification model because it predicts the probability of a binary outcome, where it predicts if the petitioner, given the 'bag of words', won or lost the case.\n",
    "\n",
    "It is also worth noting that regularization can also be added to the model to prevent overfitting, but we are not currently using this feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**What can we infer using logistic regression on our dataset?**\n",
    "\n",
    "We want to infer the probability of a petitioner winning a case given the 'bag of words' from the oral arguments. We can train a logistic regression model on the 'bag of words' of the cases pertaining the training dataset. We can then use such model to make predictions on cases not seen by the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**What were our findings using logistic regression?**\n",
    "\n",
    "We applied logistic regression models to four different datasets and observed the following results:\n",
    "\n",
    "All Utterances: This dataset comprises a bag of words created from all utterances in the cases between 2014 and 2019, including judge, advocate, and adversary statements. The model achieved an accuracy of 54.05%, slightly better than random chance.\n",
    "\n",
    "Judge Utterances: This dataset focuses on a bag of words from judge statements only. The model's accuracy was 52.05%, underperforming compared to the model using all utterances.\n",
    "\n",
    "Advocate Utterances: Using a bag of words created solely from advocate statements, the model achieved an accuracy of 75.67%, significantly outperforming the model using all utterances.\n",
    "\n",
    "Adversary Utterances: Similarly, this dataset consists of a bag of words derived only from adversary statements. The model obtained an accuracy of 78.37%, outperforming all other models and emerging as the best performer.\n",
    "\n",
    "In conclusion, models using advocate and adversary utterances independently have higher predictive power compared to models using judge utterances or a combination of all utterances.\n",
    "\n",
    "\n",
    "```\n",
    "------------------------------------------\n",
    "Running regression on total_utterances...\n",
    "Creating bag of words\n",
    "Starting the Logistic Regression\n",
    "Accuracy score: 0.5405405405405406\n",
    "------------------------------------------\n",
    "------------------------------------------\n",
    "Running regression on judge_utterances...\n",
    "Creating bag of words\n",
    "Starting the Logistic Regression\n",
    "Accuracy score: 0.5205479452054794\n",
    "------------------------------------------\n",
    "------------------------------------------\n",
    "Running regression on advocate_utterances...\n",
    "Creating bag of words\n",
    "Starting the Logistic Regression\n",
    "Accuracy score: 0.7567567567567568\n",
    "------------------------------------------\n",
    "------------------------------------------\n",
    "Running regression on adversary_utterances...\n",
    "Creating bag of words\n",
    "Starting the Logistic Regression\n",
    "Accuracy score: 0.7837837837837838\n",
    "------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'm going to add charts for each regression model [Michael]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "DataCleaner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file houses the class that is used to clean the convokit data and convert\n",
    "it to a usable format.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from convokit import Corpus, download\n",
    "\n",
    "from supreme_court_predictions.util.contants import (\n",
    "    ENCODING_UTF_8,\n",
    "    FILE_MODE_READ,\n",
    "    LATEST_YEAR,\n",
    ")\n",
    "from supreme_court_predictions.util.files import get_full_data_pathway\n",
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"\n",
    "    This class houses the functions needed to clean the convokit data and turn\n",
    "    it into a usable format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cases_df = None\n",
    "        self.clean_case_ids = None  # stores the case IDs to use\n",
    "        self.clean_utterances_list = None\n",
    "        self.speakers_df = None\n",
    "        self.utterances_df = None\n",
    "\n",
    "        # Get local directory\n",
    "        self.local_path = get_full_data_pathway(\"convokit/\")\n",
    "        print(f\"Working in {self.local_path}\")\n",
    "\n",
    "        # Set output path\n",
    "        self.output_path = get_full_data_pathway(\"clean_convokit/\")\n",
    "        print(f\"Data will be saved to {self.output_path}\")\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Loads and outputs the Supreme Court Corpus data.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Loading Supreme Court Corpus Data...\")\n",
    "        corpus = Corpus(filename=download(\"supreme-corpus\"))\n",
    "        corpus.dump(\"supreme_corpus\", base_path=self.local_path)\n",
    "\n",
    "    # Begin reading data\n",
    "    def load_data(self, file_name):\n",
    "        \"\"\"\n",
    "        Opens the data and returns it as a dictionary.\n",
    "\n",
    "        :param file_name: The name of the file to open\n",
    "        :return: The data as a dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        path = self.local_path + f\"supreme_corpus/{file_name}\"\n",
    "        if \"jsonl\" in file_name:\n",
    "            data = []\n",
    "            with open(\n",
    "                path, encoding=ENCODING_UTF_8, mode=FILE_MODE_READ\n",
    "            ) as json_file:\n",
    "                json_list = list(json_file)\n",
    "\n",
    "            for json_str in json_list:\n",
    "                clean_json = json.loads(json_str)\n",
    "                data.append(clean_json)\n",
    "        else:\n",
    "            with open(\n",
    "                path, encoding=ENCODING_UTF_8, mode=FILE_MODE_READ\n",
    "            ) as file:\n",
    "                data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def get_cases_df(self, cases_lst):\n",
    "        \"\"\"\n",
    "        Converts the cases list to a metadata dataframe. Also provides list of\n",
    "        cleaned and filtered cases to use.\n",
    "\n",
    "        :param cases_lst: The cases' list containing dictionaries of cases.\n",
    "        :return: The cases dataframe of case metadata.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to dataframe\n",
    "        cases_df = self.load_cases_df(cases_lst)\n",
    "\n",
    "        # Clean, filter, and return dataframe data\n",
    "        self.clean_case_ids = self.get_clean_cases(cases_df)\n",
    "        clean_cases_df = cases_df.loc[\n",
    "            (cases_df.loc[:, \"id\"].isin(self.clean_case_ids)), :\n",
    "        ]\n",
    "\n",
    "        clean_cases_df = clean_cases_df.astype({\"win_side\": \"int32\"})\n",
    "        return clean_cases_df\n",
    "\n",
    "    @staticmethod\n",
    "    def load_cases_df(cases_lst):\n",
    "        \"\"\"\n",
    "        Generates and unclean and unfiltered dataframe of court cases.\n",
    "\n",
    "        :param cases_lst: The cases' list containing dictionaries of cases.\n",
    "        :return: The uncleaned/unfiltered cases dataframe of case metadata.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to dataframe\n",
    "        metadata = {\n",
    "            \"id\": [],\n",
    "            \"year\": [],\n",
    "            \"citation\": [],\n",
    "            \"title\": [],\n",
    "            \"petitioner\": [],\n",
    "            \"respondent\": [],\n",
    "            \"docket_no\": [],\n",
    "            \"court\": [],\n",
    "            \"decided_date\": [],\n",
    "            \"win_side\": [],\n",
    "            \"is_eq_divided\": [],\n",
    "        }\n",
    "\n",
    "        for case in cases_lst:\n",
    "            # get metadata\n",
    "            for attr, obvs in metadata.items():\n",
    "                obvs.append(case[attr])\n",
    "\n",
    "        return pd.DataFrame(metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_clean_cases(cases):\n",
    "        \"\"\"\n",
    "        Generates a list of cleaned case IDs.\n",
    "\n",
    "        :param cases: An uncleaned dataframe of cases.\n",
    "        : return: A list of clean case IDs\n",
    "        \"\"\"\n",
    "\n",
    "        # Clean cases to 0/1 win side and cases from the last 5 years\n",
    "        case_ids = cases.loc[\n",
    "            (\n",
    "                (cases.loc[:, \"win_side\"] == 0.0)\n",
    "                | (cases.loc[:, \"win_side\"] == 1.0)\n",
    "            )\n",
    "            & (cases.loc[:, \"year\"] >= LATEST_YEAR - 5),\n",
    "            \"id\",\n",
    "        ].unique()\n",
    "\n",
    "        return case_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def speakers_to_df(speakers_dict):\n",
    "        \"\"\"\n",
    "        Converts the speakers dictionary to a pandas dataframe.\n",
    "\n",
    "        :param speakers_dict: The speaker's dictionary\n",
    "        :return: The speakers dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        dict_list = []\n",
    "        for speaker_key in list(speakers_dict.keys()):\n",
    "            speaker_data = speakers_dict[speaker_key][\"meta\"]\n",
    "            speaker_data[\"speaker_key\"] = re.sub(r\"^j__\", \"\", speaker_key)\n",
    "            dict_list.append(speaker_data)\n",
    "\n",
    "        df = pd.DataFrame(dict_list)\n",
    "        df.rename(\n",
    "            columns={\n",
    "                \"name\": \"speaker_name\",\n",
    "                \"type\": \"speaker_type\",\n",
    "                \"role\": \"speaker_role\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Remove low-quality data - unknown speaker types\n",
    "        df_cleaned = df.loc[(df.loc[:, \"speaker_type\"] != \"U\"), :]\n",
    "\n",
    "        return df_cleaned\n",
    "\n",
    "    def get_conversation_dfs(self, conversations_dict):\n",
    "        \"\"\"\n",
    "        Converts the conversations dictionary to several pandas dataframes.\n",
    "\n",
    "        :param conversations_dict: The conversations' dictionary\n",
    "        :return: The conversations dataframe, advocates dataframe,\n",
    "                and voters dataframe\n",
    "        \"\"\"\n",
    "        metadata_list = []\n",
    "        advocates_list = []\n",
    "        voters_list = []\n",
    "\n",
    "        for conversation_id in list(conversations_dict.keys()):\n",
    "            conversation_data = conversations_dict[conversation_id][\"meta\"]\n",
    "\n",
    "            # Filter dataset based on cleaned case ids and 0/1 side\n",
    "            if conversation_data[\"case_id\"] in self.clean_case_ids:\n",
    "                clean_dict = {\n",
    "                    \"id\": conversation_id,\n",
    "                    \"case_id\": conversation_data[\"case_id\"],\n",
    "                    \"winning_side\": conversation_data[\"win_side\"],\n",
    "                }\n",
    "\n",
    "                advocates = conversation_data[\"advocates\"]\n",
    "                voters = conversation_data[\"votes_side\"]\n",
    "\n",
    "                for advocate in advocates:\n",
    "                    if advocates[advocate][\"side\"] in [0, 1]:\n",
    "                        advocate_dict = {\n",
    "                            \"id\": conversation_id,\n",
    "                            \"case_id\": conversation_data[\"case_id\"],\n",
    "                            \"advocate\": advocate,\n",
    "                            \"side\": advocates[advocate][\"side\"],\n",
    "                            \"role\": advocates[advocate][\"role\"],\n",
    "                        }\n",
    "                        advocates_list.append(advocate_dict)\n",
    "\n",
    "                if voters:\n",
    "                    for voter, vote in voters.items():\n",
    "                        if vote in [0, 1]:\n",
    "                            vote_dict = {\n",
    "                                \"id\": conversation_id,\n",
    "                                \"case_id\": conversation_data[\"case_id\"],\n",
    "                                \"voter\": voter,\n",
    "                                \"vote\": vote,\n",
    "                            }\n",
    "                            voters_list.append(vote_dict)\n",
    "                else:\n",
    "                    vote_dict = {\n",
    "                        \"id\": conversation_id,\n",
    "                        \"case_id\": conversation_data[\"case_id\"],\n",
    "                    }\n",
    "                    voters_list.append(vote_dict)\n",
    "\n",
    "                metadata_list.append(clean_dict)\n",
    "\n",
    "        conversation_metadata_df = pd.DataFrame(metadata_list)\n",
    "        advocates_df = pd.DataFrame(advocates_list)\n",
    "        voters_df = pd.DataFrame(voters_list)\n",
    "\n",
    "        # Clean voters df - one vote per voter per case\n",
    "        voters_df = voters_df.drop_duplicates(\n",
    "            subset=[\"case_id\", \"voter\"], keep=\"last\"\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        return conversation_metadata_df, advocates_df, voters_df\n",
    "\n",
    "    def clean_utterances(self, utterances_list):\n",
    "        \"\"\"\n",
    "        Cleans the utterances list.\n",
    "\n",
    "        :param utterances_list: The utterances list\n",
    "        :return: The cleaned utterances list\n",
    "        \"\"\"\n",
    "\n",
    "        # Filter dataset based on cleaned case ids\n",
    "        utterances_list_filtered = [\n",
    "            u\n",
    "            for u in utterances_list\n",
    "            if u[\"meta\"][\"case_id\"] in self.clean_case_ids\n",
    "        ]\n",
    "\n",
    "        clean_utterances_list = []\n",
    "        for utterance in utterances_list_filtered:\n",
    "            clean_dict = {\n",
    "                \"case_id\": utterance[\"meta\"][\"case_id\"],\n",
    "                \"speaker\": utterance[\"speaker\"],\n",
    "                \"speaker_type\": utterance[\"meta\"][\"speaker_type\"],\n",
    "                \"conversation_id\": utterance[\"conversation_id\"],\n",
    "                \"id\": utterance[\"id\"],\n",
    "            }\n",
    "            utterance_text = utterance[\"text\"]\n",
    "            clean_utterance = utterance_text.lower()\n",
    "            no_newline = re.sub(r\"[\\r\\n\\t]\", \" \", clean_utterance)\n",
    "            no_bracket = re.sub(r\"[\\[\\]\\(\\)]\", \"\", no_newline)\n",
    "\n",
    "            clean_dict[\"text\"] = no_bracket\n",
    "\n",
    "            clean_utterances_list.append(clean_dict)\n",
    "\n",
    "        utterances_df = pd.DataFrame(clean_utterances_list)\n",
    "\n",
    "        return clean_utterances_list, utterances_df\n",
    "\n",
    "    def parse_all_data(self):\n",
    "        \"\"\"\n",
    "        Cleans and parses all the data.\n",
    "        \"\"\"\n",
    "        print(\"Parsing cases...\")\n",
    "        cases_list = self.load_data(\"cases.jsonl\")\n",
    "        self.cases_df = self.get_cases_df(cases_list)\n",
    "\n",
    "        print(\"Parsing speakers...\")\n",
    "        speakers_dict = self.load_data(\"speakers.json\")\n",
    "        self.speakers_df = self.speakers_to_df(speakers_dict)\n",
    "\n",
    "        print(\"Parsing conversations metadata...\")\n",
    "        conversations_dict = self.load_data(\"conversations.json\")\n",
    "        (\n",
    "            self.conversations_df,\n",
    "            self.advocates_df,\n",
    "            self.voters_df,\n",
    "        ) = self.get_conversation_dfs(conversations_dict)\n",
    "\n",
    "        print(\"Parsing utterances...\")\n",
    "        utterances_list = self.load_data(\"utterances.jsonl\")\n",
    "        self.clean_utterances_list, self.utterances_df = self.clean_utterances(\n",
    "            utterances_list\n",
    "        )\n",
    "\n",
    "        self.speakers_df.to_csv(\n",
    "            self.output_path + \"/speakers_df.csv\",\n",
    "            index=False,\n",
    "            encoding=ENCODING_UTF_8,\n",
    "        )\n",
    "        self.conversations_df.to_csv(\n",
    "            self.output_path + \"/conversations_df.csv\",\n",
    "            index=False,\n",
    "            encoding=ENCODING_UTF_8,\n",
    "        )\n",
    "        self.advocates_df.to_csv(\n",
    "            self.output_path + \"/advocates_df.csv\",\n",
    "            index=False,\n",
    "            encoding=ENCODING_UTF_8,\n",
    "        )\n",
    "        self.voters_df.to_csv(\n",
    "            self.output_path + \"/voters_df.csv\",\n",
    "            index=False,\n",
    "            encoding=ENCODING_UTF_8,\n",
    "        )\n",
    "        self.utterances_df.to_csv(\n",
    "            self.output_path + \"/utterances_df.csv\",\n",
    "            index=False,\n",
    "            encoding=ENCODING_UTF_8,\n",
    "        )\n",
    "        self.cases_df.to_csv(\n",
    "            self.output_path + \"/cases_df.csv\",\n",
    "            index=False,\n",
    "            encoding=ENCODING_UTF_8,\n",
    "        )\n",
    "\n",
    "        print(\"Data saved to \" + self.output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tokenizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module implementing a Tokenizer class that tokenizes text.\n",
    "The Tokenizer class provides methods for initializing the required\n",
    "components and processing text data. It is designed to handle tokenization\n",
    "tasks efficiently and effectively in a user-friendly manner.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from supreme_court_predictions.util.files import get_full_data_pathway\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer class that uses the spaCy library for\n",
    "    tokenizing / lemmatizing text. This class initializes and provides\n",
    "    methods for tokenizing text.\n",
    "    \"\"\"\n",
    "\n",
    "    SPACY_PACKAGE = \"en_core_web_sm\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Tokenizer class by setting up the local path\n",
    "        and loading the spaCy model.\n",
    "        \"\"\"\n",
    "        # Get local directory\n",
    "        self.local_path = get_full_data_pathway(\"clean_convokit/\")\n",
    "        print(f\"Data will be saved to: \\n{self.local_path}\")\n",
    "\n",
    "        try:\n",
    "            self.nlp = spacy.load(self.SPACY_PACKAGE, disable=[\"parser\", \"ner\"])\n",
    "        except OSError:\n",
    "            print(\"Spacy not present. Downloading files.\")\n",
    "            spacy.cli.download(self.SPACY_PACKAGE)\n",
    "            self.nlp = spacy.load(self.SPACY_PACKAGE, disable=[\"parser\", \"ner\"])\n",
    "        print(\"Spacy module successfully loaded.\")\n",
    "\n",
    "        self.tokenize()\n",
    "\n",
    "    def spacy_apply(self, text):\n",
    "        \"\"\"\n",
    "        Applies the spaCy tokenizer on the input text\n",
    "        and returns a list of tokens.\n",
    "\n",
    "        :param text: Input text to tokenize.\n",
    "        :return: List of tokenized words.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        return [\n",
    "            token.lemma_\n",
    "            for token in doc\n",
    "            if token.is_alpha and not token.is_stop\n",
    "        ]\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"\n",
    "        Tokenizes the text in the utterances DataFrame\n",
    "        and saves the result as a new CSV file.\n",
    "        \"\"\"\n",
    "        utterances_df = pd.read_csv(self.local_path + \"utterances_df.csv\")\n",
    "\n",
    "        utterances_df[\"tokens\"] = (\n",
    "            utterances_df.loc[:, \"text\"].astype(str).apply(self.spacy_apply)\n",
    "        )\n",
    "        utterances_df.to_csv(self.local_path + \"utterances_df.csv\", index=False)\n",
    "        utterances_df.to_pickle(self.local_path + \"utterances_df.p\")\n",
    "        print(\"Spacy tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Token Aggregation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file provides aggregate tokens per case.\n",
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from supreme_court_predictions.util.files import get_full_data_pathway\n",
    "\n",
    "\n",
    "class TokenAggregations:\n",
    "    \"\"\"\n",
    "    TODO: Need document string\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.all_tokens = None\n",
    "        self.advocate_tokens = None\n",
    "        self.adversary_tokens = None\n",
    "        self.judge_tokens = None\n",
    "\n",
    "        # Get local directory\n",
    "        self.local_path = get_full_data_pathway(\"clean_convokit/\")\n",
    "        print(f\"Working in {self.local_path}\")\n",
    "\n",
    "        # Set output path\n",
    "        self.output_path = get_full_data_pathway(\"processed/\")\n",
    "        print(f\"Data will be saved to {self.output_path}\")\n",
    "\n",
    "        # Get advocate and voter side dataframes\n",
    "        self.win_side = self.get_win_side()\n",
    "        self.vote_side = self.get_vote_side()\n",
    "        self.advocate_side = self.get_advocate_side()\n",
    "        self.utterances = self.get_utterances()\n",
    "        self.utterance_sides = self.append_side(self.utterances)\n",
    "\n",
    "    def get_utterances(self):\n",
    "        \"\"\"\n",
    "        Load the utterances dataframe. Keep only the relevant columns, i.e.,\n",
    "        \"case_id\", \"speaker\", \"tokens\", and \"speaker_type\".\n",
    "\n",
    "        :return A dataframe of case utterances.\n",
    "        \"\"\"\n",
    "        utterances = pd.read_csv(self.local_path + \"utterances_df.csv\")\n",
    "        return utterances.loc[\n",
    "            :, [\"case_id\", \"speaker\", \"tokens\", \"speaker_type\"]\n",
    "        ]\n",
    "\n",
    "    def get_win_side(self):\n",
    "        \"\"\"\n",
    "        Get the winning side of the cases.\n",
    "\n",
    "        :return A dataframe containing case IDs and the winning side (0=for\n",
    "                respondent, 1=for petitioner)\n",
    "        \"\"\"\n",
    "        win_side_df = pd.read_csv(self.local_path + \"cases_df.csv\").rename(\n",
    "            columns={\"id\": \"case_id\"}\n",
    "        )\n",
    "        return win_side_df.loc[:, [\"case_id\", \"win_side\"]]\n",
    "\n",
    "    def get_vote_side(self):\n",
    "        \"\"\"\n",
    "        Get the voting side of the cases (for Judges only).\n",
    "\n",
    "        :return A dataframe containing case IDs and the voting side (0=for\n",
    "                respondent, 1=for petitioner)\n",
    "        \"\"\"\n",
    "        vote_side_df = pd.read_csv(self.local_path + \"voters_df.csv\")\n",
    "        vote_side_df = vote_side_df.rename(\n",
    "            columns={\"vote\": \"side\", \"voter\": \"advocate\"}\n",
    "        )\n",
    "        return vote_side_df.loc[:, [\"case_id\", \"advocate\", \"side\"]]\n",
    "\n",
    "    def get_advocate_side(self):\n",
    "        \"\"\"\n",
    "        Get the advocating side of the cases (for non-Judges only).\n",
    "\n",
    "        :return A dataframe containing case IDs and the advocating side (0=for\n",
    "                respondent, 1=for petitioner)\n",
    "        \"\"\"\n",
    "        advocate_side_df = pd.read_csv(self.local_path + \"advocates_df.csv\")\n",
    "        return advocate_side_df.loc[:, [\"case_id\", \"advocate\", \"side\"]]\n",
    "\n",
    "    def get_case_tokens(self, utterance_tokens):\n",
    "        \"\"\"\n",
    "        Returns a dataframe of utterances tokens per case, including win_side of\n",
    "        the case. The utterance_tokens dataframe is expected to have the columns\n",
    "        \"case_id\" and \"tokens\".\n",
    "\n",
    "        :param utterance_tokens: A dataframe of case utterances to aggregate\n",
    "            tokens of.\n",
    "        \"\"\"\n",
    "        # Aggregate tokens by case_id\n",
    "        case_ids = utterance_tokens.loc[:, \"case_id\"].unique()\n",
    "        agg_tokens = {\"case_id\": [], \"tokens\": []}\n",
    "\n",
    "        for case in case_ids:\n",
    "            tokens = []\n",
    "            agg_tokens[\"case_id\"].append(case)\n",
    "            for token in utterance_tokens.loc[\n",
    "                utterance_tokens.loc[:, \"case_id\"] == case, \"tokens\"\n",
    "            ]:\n",
    "                # Preprocess instances - from string to list\n",
    "                token = token.strip(\"[\")\n",
    "                token = token.strip(\"]\")\n",
    "                token = token.replace(\"'\", \"\")\n",
    "                token = token.replace(\" \", \"\")\n",
    "                token = token.split(\",\")\n",
    "\n",
    "                tokens.extend(token)\n",
    "\n",
    "            agg_tokens[\"tokens\"].append(tokens)\n",
    "\n",
    "        agg_tokens = pd.DataFrame.from_dict(agg_tokens)\n",
    "\n",
    "        # merging win_side onto tokens\n",
    "        agg_tokens_win_side = pd.merge(\n",
    "            agg_tokens, self.win_side, how=\"left\", on=\"case_id\"\n",
    "        )\n",
    "\n",
    "        return agg_tokens_win_side\n",
    "\n",
    "    def get_all_case_tokens(self):\n",
    "        \"\"\"\n",
    "        Gets all the tokens for a given case and the outcome of the case.\n",
    "\n",
    "        :return A dataframe of case utterances to aggregate tokens of.\n",
    "        \"\"\"\n",
    "        return self.get_case_tokens(\n",
    "            self.utterance_sides.loc[:, [\"case_id\", \"tokens\"]]\n",
    "        )\n",
    "\n",
    "    def append_side(self, utterances):\n",
    "        \"\"\"\n",
    "        Adds the speaker's advocate or vote side to the utterances dataframe.\n",
    "\n",
    "        :param utterances: The utterances dataframe; must have only the columns\n",
    "            case_id, speaker, speaker_type, and tokens\n",
    "\n",
    "        :returns A dataframe with the side of the speaker appended to\n",
    "                 utterances, also removing speaker accounts who don't have a\n",
    "                 side.\n",
    "        \"\"\"\n",
    "        # Renaming the speaker column for easier merging\n",
    "        ut = utterances.rename(columns={\"speaker\": \"advocate\"})\n",
    "\n",
    "        # Merging utterances with voter (judge) and advocate sides\n",
    "        ut_sides = pd.merge(\n",
    "            ut,\n",
    "            pd.concat([self.vote_side, self.advocate_side]),\n",
    "            how=\"left\",\n",
    "            left_on=[\"case_id\", \"advocate\"],\n",
    "            right_on=[\"case_id\", \"advocate\"],\n",
    "        )\n",
    "\n",
    "        # Remove NA values\n",
    "        ut_sides = ut_sides.loc[~ut_sides.loc[:, \"side\"].isna(), :]\n",
    "        ut_sides = ut_sides.astype({\"side\": \"int32\"})\n",
    "\n",
    "        return ut_sides\n",
    "\n",
    "    def get_advocate_case_tokens(self, advocate=True):\n",
    "        \"\"\"\n",
    "        Get all tokens for individuals either in favor of the petitioner or\n",
    "        opposed to the petitioner.\n",
    "\n",
    "        :param advocate: Whether to find the tokens for those in favor\n",
    "                         (advocate=True) or opposed (advocate=False)\n",
    "        :return A dataframe of tokens per case for petitioner advocates.\n",
    "        \"\"\"\n",
    "        if advocate:\n",
    "            ut = self.utterance_sides.loc[\n",
    "                self.utterance_sides.loc[:, \"side\"] == 1, :\n",
    "            ]\n",
    "        else:\n",
    "            ut = self.utterance_sides.loc[\n",
    "                self.utterance_sides.loc[:, \"side\"] == 0, :\n",
    "            ]\n",
    "        return self.get_case_tokens(ut.loc[:, [\"case_id\", \"tokens\"]])\n",
    "\n",
    "    def get_judge_case_tokens(self):\n",
    "        \"\"\"\n",
    "        Get all of the tokens for only judges.\n",
    "\n",
    "        :return A dataframe of tokens per judge per case.\n",
    "        \"\"\"\n",
    "        ut = self.utterance_sides.loc[\n",
    "            self.utterance_sides.loc[:, \"speaker_type\"] == \"J\", :\n",
    "        ]\n",
    "        return self.get_case_tokens(ut.loc[:, [\"case_id\", \"tokens\"]])\n",
    "\n",
    "    def parse_all_data(self):\n",
    "        \"\"\"\n",
    "        Generates token aggregations for 1) all speakers, 2) only advocates, 3)\n",
    "        only adversaries, 4) only judges, and appends the winning side of the\n",
    "        case. DataFrames and exports them as pickle objects (if applicable).\n",
    "        \"\"\"\n",
    "        print(\"Grabbing token aggregation for all cases...\")\n",
    "        self.all_tokens = self.get_all_case_tokens()\n",
    "\n",
    "        print(\"Grabbing token aggregation for advocates...\")\n",
    "        self.advocate_tokens = self.get_advocate_case_tokens(True)\n",
    "\n",
    "        print(\"Grabbing token aggregation for adversaries...\")\n",
    "        self.adversary_tokens = self.get_advocate_case_tokens(False)\n",
    "\n",
    "        print(\"Grabbing token aggregation for judges...\")\n",
    "        self.judge_tokens = self.get_judge_case_tokens()\n",
    "\n",
    "        print(\"Exporting files...\")\n",
    "        # Outputting to CSVs\n",
    "        aggregations = [\n",
    "            self.all_tokens,\n",
    "            self.advocate_tokens,\n",
    "            self.adversary_tokens,\n",
    "            self.judge_tokens,\n",
    "        ]\n",
    "        output_paths = [\n",
    "            self.output_path + \"/case_aggregations.p\",\n",
    "            self.output_path + \"/advocate_aggregations.p\",\n",
    "            self.output_path + \"/adversary_aggregations.p\",\n",
    "            self.output_path + \"/judge_aggregations.p\",\n",
    "        ]\n",
    "\n",
    "        for idx, agg in enumerate(aggregations):\n",
    "            pickle.dump(agg, open(output_paths[idx], \"wb\"))\n",
    "\n",
    "        print(\"Data saved to \" + self.output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the class that is the basis for all models in this package.\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"\n",
    "    This class sets basis for what makes up a model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def create(df):\n",
    "        \"\"\"\n",
    "        Creates the model and returns an accuracy score.\n",
    "\n",
    "        :param df: A dataframe used to create the model.\n",
    "        :return: A sklearn model.\n",
    "        \"\"\"\n",
    "\n",
    "    def create_and_measure(self, df, accuracy_measure):\n",
    "        \"\"\"\n",
    "        Takes in a dataframe and returns the applicable accuracy measurement.\n",
    "\n",
    "        :param df: A dataframe used to create the model.\n",
    "        :param accuracy_measure: A function that is used to measure accuracy\n",
    "        on the model.\n",
    "        :return: Float of some accuracy measurement.\n",
    "        \"\"\"\n",
    "        _, y_test, y_pred = self.create(df)\n",
    "\n",
    "        return accuracy_measure(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the model on its respective data.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This LogisticRegression class runs logistic regression\n",
    "on utterance data from the Supreme Court dataset. This class aims to predict\n",
    "the results of a case based on the text learned from utterances.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as skLR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from supreme_court_predictions.models.model import Model\n",
    "from supreme_court_predictions.util.files import get_full_data_pathway\n",
    "\n",
    "\n",
    "class LogisticRegression(Model):\n",
    "    \"\"\"\n",
    "    A class that runs logistic regression on aggregated utterance and cases data\n",
    "    from the Supreme Court dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.local_path = get_full_data_pathway(\"processed/\")\n",
    "\n",
    "        self.total_utterances = pd.read_pickle(\n",
    "            self.local_path + \"case_aggregations.p\"\n",
    "        )\n",
    "        self.advocate_utterances = pd.read_pickle(\n",
    "            self.local_path + \"advocate_aggregations.p\"\n",
    "        )\n",
    "        self.adversary_utterances = pd.read_pickle(\n",
    "            self.local_path + \"adversary_aggregations.p\"\n",
    "        )\n",
    "        self.judge_utterances = pd.read_pickle(\n",
    "            self.local_path + \"judge_aggregations.p\"\n",
    "        )\n",
    "\n",
    "        self.run()\n",
    "\n",
    "    @staticmethod\n",
    "    def create(df):\n",
    "        \"\"\"\n",
    "        Creates and runs a logistic regression on the given dataframe of\n",
    "        utterance data.\n",
    "\n",
    "        :param df: DataFrame containing utterance data\n",
    "\n",
    "        :return (regressor, y_test, y_pred): A tuple that contains the\n",
    "        regression model, test y-data, the predicted y-data.\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\", max_features=5000)\n",
    "        vectorize_document = df.loc[:, \"tokens\"].apply(\" \".join)\n",
    "        print(\"Creating bag of words\")\n",
    "        bag_of_words_x = vectorizer.fit_transform(vectorize_document)\n",
    "\n",
    "        bag_of_words_y = df.loc[:, \"win_side\"]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            bag_of_words_x,\n",
    "            bag_of_words_y,\n",
    "            test_size=0.20,\n",
    "            random_state=123,\n",
    "            stratify=bag_of_words_y,\n",
    "        )\n",
    "\n",
    "        print(\"Starting the Logistic Regression\")\n",
    "        regressor = skLR(max_iter=1000)\n",
    "\n",
    "        # Fit the classifier on the training data\n",
    "        regressor.fit(X_train, y_train)\n",
    "        y_pred = regressor.predict(X_test)\n",
    "\n",
    "        return regressor, y_test, y_pred\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the create function on each type of aggregated utterance.\n",
    "        \"\"\"\n",
    "\n",
    "        dfs = [\n",
    "            (\"total_utterances\", self.total_utterances),\n",
    "            (\"judge_utterances\", self.judge_utterances),\n",
    "            (\"advocate_utterances\", self.advocate_utterances),\n",
    "            (\"adversary_utterances\", self.adversary_utterances),\n",
    "        ]\n",
    "\n",
    "        for df_name, df in dfs:\n",
    "            try:\n",
    "                print(\"------------------------------------------\")\n",
    "                print(f\"Running regression on {df_name}...\")\n",
    "                acc = self.create_and_measure(df, accuracy_score)\n",
    "                print(f\"Accuracy score: {acc}\")\n",
    "                print(\"------------------------------------------\")\n",
    "            except ValueError:\n",
    "                print(\"------------------------------------------\")\n",
    "                print(\"Error: training data is not big enough for this subset\")\n",
    "                print(\"------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
